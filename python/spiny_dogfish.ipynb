{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions for model \n",
    "%run model_functions.ipynb\n",
    "# %run jude_plot_code.py # not doing plotting anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from __future__ import print_function, division\n",
    "import sys\n",
    "import numpy as np\n",
    "import random as random\n",
    "import math as math\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from random import triangular\n",
    "import scipy.stats as sst\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import jude_plot_code as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rejection\n",
      "in_situ\n"
     ]
    }
   ],
   "source": [
    "# set code preferences for spiny dogfish model\n",
    "year_split = 2008 # in what year should the data be split into estimation (below year_split) or validation (equals to or after year_split)? currently leaving 10 years for forecasting/validation\n",
    "abc_options = ['regression','rejection']\n",
    "abc_pref = abc_options[1] # choose which approach to Approximate Bayesian Computation to use \n",
    "temperature_options = ['ROMS','in_situ'] # ROMS is still the incorrect run from the cod forecast challenge\n",
    "temperature_pref = temperature_options[1]\n",
    "print(abc_pref) # check it's correct, and remember Python starts from 0! \n",
    "print(temperature_pref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['haulid',\n",
       " 'lengthclass',\n",
       " 'spp',\n",
       " 'numlengthclass',\n",
       " 'region',\n",
       " 'year',\n",
       " 'common',\n",
       " 'stratum',\n",
       " 'stratumarea',\n",
       " 'lat',\n",
       " 'lon',\n",
       " 'depth',\n",
       " 'btemp']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import spiny dogfish data & look at columns \n",
    "dat_trawl = pd.read_csv(\"../processed-data/dogfish_prepped_data.csv\")\n",
    "list(dat_trawl.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['haulid', 'numlengthclass', 'year', 'lat', 'lengthclass', 'temp_bottom']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep only needed columns and import temperature data if needed \n",
    "if temperature_pref=='ROMS':\n",
    "    dat_trawl = dat_trawl[['haulid','numlengthclass', 'year', 'lat','lengthclass']] # keep only needed columns \n",
    "    dat_roms = pd.read_csv(\"~/github/SDM-convergence/data/haul_ROMS.csv\", usecols = ['unique_id',  'temp_bottom', 'temp_surface']) # import ROMS data \n",
    "    dat_roms.rename({\"unique_id\":\"haulid\"},axis=\"columns\",inplace=True) # fix column names \n",
    "    dat_trawl = pd.merge(dat_estimation, dat_roms, how=\"inner\", on=\"haulid\") # merge with trawl data. because this is an inner join, it will omit NOAA hauls with no ROMS data, and ROMS data with no matches in the species' survey dataframe\n",
    "if temperature_pref=='in_situ':\n",
    "    dat_trawl = dat_trawl[['haulid','numlengthclass', 'year', 'lat','lengthclass','btemp']]\n",
    "    dat_trawl.rename({'btemp':'temp_bottom'},axis=\"columns\",inplace=True)\n",
    "list(dat_trawl.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1963\n",
      "2018\n",
      "1963\n",
      "2008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/afh/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:5303: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "# filter trawl data \n",
    "dat_estimation = dat_trawl.loc[(dat_trawl['year'] <= year_split)] # use years before year_split for estimation\n",
    "\n",
    "# check year filtering worked correctly\n",
    "print(dat_trawl.year.min())\n",
    "print(dat_trawl.year.max())\n",
    "\n",
    "print(dat_estimation.year.min())\n",
    "print(dat_estimation.year.max())\n",
    "\n",
    "# round latitudes to integers\n",
    "dat_estimation.lat = dat_estimation.lat.round().astype(np.int) # revisit and be more precise about rounding; currently rounding to nearest integer, so bands are defined as center points (35-degree band runs from 34.51 to 35.49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking care of missing data in both data frames\n",
    "# AF: commenting this out because interpolation is a bit dodgy here. will just deal with NAs \n",
    "# dat_estimation=dat_estimation.interpolate(method ='linear', limit_direction ='forward')\n",
    "# dat_estimation=dat_estimation.interpolate(method ='linear', limit_direction ='backward')\n",
    "# dat_roms =dat_roms.interpolate(method ='linear', limit_direction ='forward')\n",
    "# dat_roms=dat_roms.interpolate(method ='linear', limit_direction ='backward')\n",
    "\n",
    "\n",
    "# USE DF = DF.DROPNA() INSTEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['haulid', 'numlengthclass', 'year', 'lat', 'lengthclass', 'temp_bottom']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dat_estimation.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "[29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47]\n",
      "56\n",
      "46\n",
      "[1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976\n",
      " 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990\n",
      " 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004\n",
      " 2005 2006 2007 2008]\n",
      "10\n",
      "[2009 2010 2011 2012 2013 2014 2015 2016 2017 2018]\n"
     ]
    }
   ],
   "source": [
    "# track number of latitudes--currently the spatial unit of analysis\n",
    "yearsN = dat_trawl['year'].nunique()\n",
    "latRange = np.arange(start=dat_estimation.lat.min(), stop=dat_estimation.lat.max()+1, step=1) # recall that the range family of functions automatically omit the \"stop\" value\n",
    "latN = latRange.size # note that this is slightly different from Jude's code, which defined nn as lat.max-lat.min, which would be my latN-1\n",
    "yearsTrainRange = np.arange(start=dat_estimation.year.min(), stop=dat_estimation.year.max()+1, step=1)\n",
    "yearsTrainN = yearsTrainRange.size\n",
    "yearsTestRange = np.arange(start=year_split+1, stop=dat_trawl.year.max()+1, step=1)\n",
    "yearsTestN = yearsTestRange.size\n",
    "\n",
    "print(latN)\n",
    "print(latRange)\n",
    "print(yearsN)\n",
    "print(yearsTrainN)\n",
    "print(yearsTrainRange)\n",
    "print(yearsTestN)\n",
    "print(yearsTestRange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of changes to nomenclature in Jude's code\n",
    "\n",
    "| Original name | New name | Meaning |\n",
    "| --- | --- | --- |\n",
    "| `nn` | `latN` | number of unique latitude values |\n",
    "| `df` | `dat_estimation` | trawl survey dataset used for training |\n",
    "| `df['NUMLEN']` | `dat_estimation['numlengthclass']` | column name |\n",
    "| `n` | `Nobs` | number of observations in a patch and age class |\n",
    "| `m` | `yearsTrainN` | number of years of data in training dataset |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/afh/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:33: RuntimeWarning: Mean of empty slice\n",
      "/Users/afh/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:72: RuntimeWarning: Mean of empty slice\n",
      "/Users/afh/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:106: RuntimeWarning: Mean of empty slice\n"
     ]
    }
   ],
   "source": [
    "# subdivide training data into a \"dictionary\" referenced by \"patch\" (latitude band) and life stage \n",
    "D={}\n",
    "D1={}\n",
    "# AF: what happens when all values are NA for mean() and sum() below? check that this isn't an issue\n",
    "\n",
    "#extracting the data from the data frames  and storing according to the patch and stage. We start from the first to the last last patch (1:nn+1)  and when in each patch, we extract the number of species for  each life stage, the temperature for each patch (and compute the avaerage for the year)\n",
    "for q in range(1,latN+1): # latN+1 because range() does not use the final value, i.e., range(1,3) equals [1,2]\n",
    "    #Juveniles for patch 33+q( since the min patch is 34, we will start with patch 34 --to the maximum # AF: get rid of all fixed numerics here \n",
    "    D['J_patch'+ str(q)]=dat_estimation.loc[(dat_estimation['lat'] == (dat_estimation['lat'].min()-1)+ q) & (dat_estimation['lengthclass']=='smalljuv')]\n",
    "    #total number of observations in each patch for each year\n",
    "    Nobs=len(D['J_patch'+ str(q)].year.values)\n",
    "    A=np.empty((Nobs, 3))\n",
    "    Abun_TemJ=np.empty((yearsTrainN, 4)) # create empty object; Alexa adding a column for real year \n",
    "    A[:,0]=D['J_patch'+ str(q)].year.values \n",
    "    A[:,1]=D['J_patch'+ str(q)].temp_bottom.values\n",
    "    A[:,2]=D['J_patch'+ str(q)].numlengthclass.values\n",
    "    kJ=0 # indices \n",
    "    kY=0\n",
    "    kA=0\n",
    "    # get temperature and abundance data in each year \n",
    "    for i in range (0, yearsTrainN):\n",
    "        Abun_TemJ[i,0]=kJ\n",
    "        N=np.zeros(len(A[:,0]))\n",
    "        T=np.zeros(len(A[:,0]))\n",
    "        for j in range (0, len(A[:,0])):\n",
    "            if A[j,0] == min(yearsTrainRange)+i:\n",
    "               T[j]=A[j,1]\n",
    "               N[j]=A[j,2]\n",
    "            else:\n",
    "               T[j]=float(\"nan\")\n",
    "               N[j]=float(\"nan\")\n",
    "     #   Abun_TemJ[i,1]=np.nan_to_num(T[T!=0].mean())# see notes in adult chunk below re: this method\n",
    "        Abun_TemJ[i,1]=np.nanmean(T)\n",
    "        Abun_TemJ[i,2]=np.nansum(N)\n",
    "        Abun_TemJ[i,3]=yearsTrainRange[i] # preserve actual year value \n",
    "        kJ=kJ +1\n",
    "        \n",
    "#         Abun_TemJ[i,0]=kJ # add index value \n",
    "#         DD=D['J_patch'+ str(q)] \n",
    "#         TT=DD.loc[(DD['year'] == yearsTrainRange[i])] # further subdivide patch data by year \n",
    "#         temp1=DD.loc[(DD['year'] == yearsTrainRange[i])] # why is this different from the above line? ask Jude\n",
    "#      #   print(temp1.temp_bottom.values) # AF: not sure why this is here \n",
    "#         Abun_TemJ[i,1]=temp1.temp_bottom.values.mean()\n",
    "#         Abun_TemJ[i,2]=TT.numlengthclass.values.sum()\n",
    "#         Abun_TemJ[i,3]=yearsTrainRange[i] # preserve actual year value \n",
    "        \n",
    "#         kJ=kJ +1\n",
    "    #After extracting teh temperature and calculating the mean value, we now save it\n",
    "    D1['J_patch'+ str(q)]=Abun_TemJ\n",
    "    \n",
    "    \n",
    "# now moving to Young Juveniles to perform teh same process as above\n",
    "    D['Y_patch'+ str(q)]=dat_estimation.loc[(dat_estimation['lat'] == (dat_estimation['lat'].min()-1)+ q) & (dat_estimation['lengthclass']=='largejuv')]\n",
    "    Nobs=len(D['Y_patch'+ str(q)].year.values)\n",
    "    B=np.empty((Nobs, 3))\n",
    "    Abun_TemY=np.empty((yearsTrainN, 4))\n",
    "    B[:,0]=D['Y_patch'+ str(q)].year.values \n",
    "    B[:,1]=D['Y_patch'+ str(q)].temp_bottom.values\n",
    "    B[:,2]=D['Y_patch'+ str(q)].numlengthclass.values\n",
    "    for i in range (0, yearsTrainN):\n",
    "        Abun_TemY[i,0]=kY\n",
    "        N=np.zeros(len(B[:,0]))\n",
    "        T=np.zeros(len(B[:,0]))\n",
    "        for j in range (0, len(B[:,0])):\n",
    "            if B[j,0] == min(yearsTrainRange)+i:\n",
    "                T[j]=B[j,1]\n",
    "                N[j]=B[j,2]\n",
    "            else:\n",
    "                T[j]=float(\"nan\")\n",
    "                N[j]=float(\"nan\")\n",
    "     #   Abun_TemY[i,1]=np.nan_to_num(T[T!=0].mean()) # see notes in adult chunk below re: this method\n",
    "        Abun_TemY[i,1]=np.nanmean(T)\n",
    "        Abun_TemY[i,2]=np.nansum(N)\n",
    "        Abun_TemY[i,3]=yearsTrainRange[i] # preserve actual year value \n",
    "\n",
    "#         DD=D['Y_patch'+ str(q)]\n",
    "#         TT=DD.loc[(DD['year'] == yearsTrainRange[i])]\n",
    "#         temp1=DD.loc[(DD['year'] == yearsTrainRange[i])]\n",
    "#         Abun_TemY[i,1]=temp1.temp_bottom.values.mean()\n",
    "#         Abun_TemY[i,2]=TT.numlengthclass.values.sum()\n",
    "#         Abun_TemY[i,3]=yearsTrainRange[i] # preserve actual year value \n",
    "        kY=kY +1\n",
    "    D1['Y_patch'+ str(q)]=Abun_TemY\n",
    "    \n",
    "    \n",
    "#Next we move to Adult and perform teh same as above\n",
    "    D['A_patch'+ str(q)]=dat_estimation.loc[(dat_estimation['lat'] == (dat_estimation['lat'].min()-1)+ q) & (dat_estimation['lengthclass']=='adult')]\n",
    "    Nobs=len(D['A_patch'+ str(q)].year.values)\n",
    "    C=np.empty((Nobs, 3))\n",
    "    Abun_TemA=np.empty((yearsTrainN, 4))\n",
    "    C[:,0]=D['A_patch'+ str(q)].year.values \n",
    "    C[:,1]=D['A_patch'+ str(q)].temp_bottom.values\n",
    "    C[:,2]=D['A_patch'+ str(q)].numlengthclass.values\n",
    "    for i in range (0, yearsTrainN):\n",
    "        Abun_TemA[i,0]=kA\n",
    "        N=np.zeros(len(C[:,2]))\n",
    "        T=np.zeros(len(C[:,1]))\n",
    "        for j in range (0, len(C[:,0])): # copy over temperature and count data only from year i; iterates over all rows \n",
    "            if C[j,0] == min(yearsTrainRange)+i:\n",
    "                T[j]=C[j,1]\n",
    "                N[j]=C[j,2]\n",
    "            else:\n",
    "                T[j]=float(\"nan\") # AF: imputing a zero here was causing erroneous temperature estimates below \n",
    "                N[j]=float(\"nan\") # not sure if this is causing any issues yet\n",
    "#        Abun_TemA[i,1]=np.nan_to_num(T[T!=0].mean()) # mean bottom temperature (***has lots of zeros bc of nan_to_num*** and creates very low values, I think there is a mistake here)\n",
    "        Abun_TemA[i,1]=np.nanmean(T) # calculate mean bottemp for patch*stage*year; previously just setting to zero if missing \n",
    "        Abun_TemA[i,2]=np.nansum(N) # calculate sum individuals for patch*stage*year; previously just setting to zero if missing \n",
    "        Abun_TemA[i,3]=yearsTrainRange[i]\n",
    "#         DD=D['A_patch'+ str(q)]\n",
    "#         TT=DD.loc[(DD['year'] == yearsTrainRange[i])]\n",
    "#         temp1=DD.loc[(DD['year'] == yearsTrainRange[i])]\n",
    "#         Abun_TemA[i,1]=temp1.temp_bottom.values.mean()\n",
    "#         Abun_TemA[i,2]=TT.numlengthclass.values.sum()\n",
    "#         Abun_TemA[i,3]=yearsTrainRange[i] # preserve actual year value \n",
    "        kA=kA +1\n",
    "    D1['A_patch'+ str(q)]=Abun_TemA\n",
    "    \n",
    "    # for future, write out D1 with correct column names so that it can be visually inspected, plotted, etc. \n",
    "    # this runs fine but I'm having trouble matching up the data in D1[patch] with the input df. right now the numbers are sane (orders of magnitude) but need to precisely compare D1 to dat_estimation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 6.71842105e+00, 0.00000000e+00, 1.96300000e+03],\n",
       "       [1.00000000e+00, 5.46216216e+00, 0.00000000e+00, 1.96400000e+03],\n",
       "       [2.00000000e+00, 5.24864865e+00, 0.00000000e+00, 1.96500000e+03],\n",
       "       [3.00000000e+00, 5.18918919e+00, 0.00000000e+00, 1.96600000e+03],\n",
       "       [4.00000000e+00, 5.51794872e+00, 0.00000000e+00, 1.96700000e+03],\n",
       "       [5.00000000e+00, 6.22051282e+00, 0.00000000e+00, 1.96800000e+03],\n",
       "       [6.00000000e+00, 6.22804878e+00, 2.00000000e+00, 1.96900000e+03],\n",
       "       [7.00000000e+00, 6.30446429e+00, 0.00000000e+00, 1.97000000e+03],\n",
       "       [8.00000000e+00, 6.63240741e+00, 0.00000000e+00, 1.97100000e+03],\n",
       "       [9.00000000e+00, 7.27327586e+00, 0.00000000e+00, 1.97200000e+03],\n",
       "       [1.00000000e+01, 7.64380952e+00, 0.00000000e+00, 1.97300000e+03],\n",
       "       [1.10000000e+01, 8.62500000e+00, 0.00000000e+00, 1.97400000e+03],\n",
       "       [1.20000000e+01, 6.94361702e+00, 0.00000000e+00, 1.97500000e+03],\n",
       "       [1.30000000e+01, 7.92772277e+00, 0.00000000e+00, 1.97600000e+03],\n",
       "       [1.40000000e+01, 6.54629630e+00, 0.00000000e+00, 1.97700000e+03],\n",
       "       [1.50000000e+01, 6.21492537e+00, 0.00000000e+00, 1.97800000e+03],\n",
       "       [1.60000000e+01, 6.63730159e+00, 0.00000000e+00, 1.97900000e+03],\n",
       "       [1.70000000e+01, 6.29516129e+00, 1.00000000e+00, 1.98000000e+03],\n",
       "       [1.80000000e+01, 6.53731343e+00, 0.00000000e+00, 1.98100000e+03],\n",
       "       [1.90000000e+01, 6.68815789e+00, 0.00000000e+00, 1.98200000e+03],\n",
       "       [2.00000000e+01, 6.97662338e+00, 0.00000000e+00, 1.98300000e+03],\n",
       "       [2.10000000e+01, 6.74150943e+00, 0.00000000e+00, 1.98400000e+03],\n",
       "       [2.20000000e+01, 7.19600000e+00, 4.00000000e+00, 1.98500000e+03],\n",
       "       [2.30000000e+01, 7.65581395e+00, 5.00000000e+00, 1.98600000e+03],\n",
       "       [2.40000000e+01, 6.13714286e+00, 1.00000000e+00, 1.98700000e+03],\n",
       "       [2.50000000e+01, 6.50833333e+00, 6.00000000e+00, 1.98800000e+03],\n",
       "       [2.60000000e+01, 6.50000000e+00, 1.80000000e+01, 1.98900000e+03],\n",
       "       [2.70000000e+01, 6.82500000e+00, 1.00000000e+00, 1.99000000e+03],\n",
       "       [2.80000000e+01, 6.92096774e+00, 1.00000000e+00, 1.99100000e+03],\n",
       "       [2.90000000e+01, 6.51272727e+00, 0.00000000e+00, 1.99200000e+03],\n",
       "       [3.00000000e+01, 6.08714286e+00, 1.00000000e+00, 1.99300000e+03],\n",
       "       [3.10000000e+01, 8.17608696e+00, 2.00000000e+00, 1.99400000e+03],\n",
       "       [3.20000000e+01, 7.20833333e+00, 5.00000000e+00, 1.99500000e+03],\n",
       "       [3.30000000e+01, 6.88870968e+00, 3.00000000e+00, 1.99600000e+03],\n",
       "       [3.40000000e+01, 7.02903226e+00, 1.00000000e+00, 1.99700000e+03],\n",
       "       [3.50000000e+01, 6.33000000e+00, 4.00000000e+00, 1.99800000e+03],\n",
       "       [3.60000000e+01, 7.25211268e+00, 1.00000000e+00, 1.99900000e+03],\n",
       "       [3.70000000e+01, 7.30714286e+00, 1.40000000e+01, 2.00000000e+03],\n",
       "       [3.80000000e+01, 6.87661017e+00, 0.00000000e+00, 2.00100000e+03],\n",
       "       [3.90000000e+01, 7.62065574e+00, 1.00000000e+01, 2.00200000e+03],\n",
       "       [4.00000000e+01, 6.01946429e+00, 1.00000000e+01, 2.00300000e+03],\n",
       "       [4.10000000e+01, 5.62833333e+00, 2.20000000e+01, 2.00400000e+03],\n",
       "       [4.20000000e+01, 6.50380952e+00, 2.80000000e+01, 2.00500000e+03],\n",
       "       [4.30000000e+01, 7.60492537e+00, 6.90000000e+01, 2.00600000e+03],\n",
       "       [4.40000000e+01, 6.37646154e+00, 3.50000000e+01, 2.00700000e+03],\n",
       "       [4.50000000e+01, 6.47266667e+00, 4.70000000e+01, 2.00800000e+03]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D1['J_patch15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS UNNECESSARY CODE TO GENERATE A LIST OF PATCH IDS, I'M JUST PROUD OF IT\n",
    "# latcodes = np.arange(start=1, stop=latN+1, step=1)\n",
    "# Alist=[]\n",
    "# Jlist=[]\n",
    "# Ylist=[]\n",
    "# for z in range(0, latN):\n",
    "#     tmpnameA = \"A_patch\"+str(latcodes[z])\n",
    "#     tmpnameJ = \"J_patch\"+str(latcodes[z])\n",
    "#     tmpnameY = \"Y_patch\"+str(latcodes[z])\n",
    "#     Alist.append(tmpnameA)\n",
    "#     Jlist.append(tmpnameJ)\n",
    "#     Ylist.append(tmpnameY)\n",
    "\n",
    "# patchcodes = Alist+Ylist+Jlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_data=[]\n",
    "# pull out D1 items into dataframes\n",
    "for patch_id, a in D1.items():\n",
    "    df = pd.DataFrame(a)\n",
    "    df['patch_id']=patch_id\n",
    "    patch_data.append(df)\n",
    "\n",
    "# collapse into one df\n",
    "D1_df = pd.concat(patch_data)\n",
    "\n",
    "# fix column names--THIS WILL BREAK IF COLUMNS ARE CHANGED\n",
    "D1_df.rename({0:'index',1:'temp_bottom',2:'abundance',3:'year'},axis=\"columns\",inplace=True)\n",
    "\n",
    "# write out to CSV\n",
    "D1_df.to_csv(\"spiny_dogfish_training_data.csv\") \n",
    "# thanks to shawn taylor @dataecologist for help converting dictionary to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that data has been structured in a dictionary for the model, run the model\n",
    "# we already imported the model functions at the top\n",
    "#main script starts from here\n",
    "# if __name__ == '__main__': # AF: ask Jude what this does -- apparently it denotes the main script--commenting out for now \n",
    "# Import the abundance data and data for the other variables e.g temperature\n",
    "#  import groundfish_training AF: this isn't necessary anymore and I deleted \"groundfish_training.\" from all the calls to D1 or D\n",
    "# the total number of generations\n",
    "T_FINAL = len(D1['J_patch1'][:,0])\n",
    "#We simulate 20000 sets of parameters for for ABC, using non informatives priors (uniform priors\n",
    "NUMBER_SIMS = 20000\n",
    "#no of patches\n",
    "no_patches=latN\n",
    "\n",
    "# need to define this for simulation_population; eventually go into functions and replace them all with sensibly named objects (T_FINAL, no_patches)\n",
    "rows=T_FINAL\n",
    "cols=no_patches\n",
    "\n",
    "# creating an array to store the number of juveniles, young juvenils and adults in each patch\n",
    "N_J=np.ndarray(shape=(rows, cols), dtype=float, order='F') # rows are years and columns are patches \n",
    "N_Y=np.ndarray(shape=(rows, cols), dtype=float, order='F')\n",
    "N_A=np.ndarray(shape=(rows, cols), dtype=float, order='F')\n",
    "tempA = np.ndarray(shape=(rows, cols), dtype=float, order='F')\n",
    "#storing data (secies abundance and temeprature time series data ) in the created arrays\n",
    "for q in range(1,no_patches+1):\n",
    "    i=q-1\n",
    "    p=q\n",
    "    N_J[:,i]=D1['J_patch'+ str(p)][:,2] # fill in the array with data from D1. column 2 in D1 holds abundance. column 0 contains indices (kJ, etc) and column 1 contains temperatures. \n",
    "    N_Y[:,i]=D1['Y_patch'+ str(p)][:,2]\n",
    "    N_A[:,i]=D1['A_patch'+ str(p)][:,2]\n",
    "    tempA[:,i]=D1['A_patch'+ str(p)][:,1] # only varies across patches, not life stages, so just need to save 1x \n",
    "#running ABC. See the function for details. returns all the observe summary statitics (OS)and simulated summary statistics (SS) in a matrix with first row corresponding to OS and the rest of the rows to SS as well as the parameter values that led to the simulated summary statistics.\n",
    "param_save, Obs_Sim         = run_sim() # runs without error for spiny dogfish; Obs_Sim has lots of zeros but maybe that's correct? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.01306985e+00 1.07694091e+02 1.21694345e+01 ... 5.86813503e-03\n",
      "  9.70000919e-02 9.42250848e+02]\n",
      " [9.14574773e-01 1.61958423e+02 1.47321088e+01 ... 8.55064527e-02\n",
      "  6.95592994e-02 7.00323089e+02]\n",
      " [8.91696669e-01 1.24934222e+02 7.72427885e+00 ... 9.74417437e-03\n",
      "  7.26823997e-02 2.98528892e+02]\n",
      " ...\n",
      " [2.20653782e+00 1.22381028e+02 8.14634423e+00 ... 5.81203215e-03\n",
      "  3.17492399e-02 4.34921623e+02]\n",
      " [1.75361468e+00 1.22232486e+02 1.07698444e+01 ... 8.72091572e-02\n",
      "  3.87422027e-02 7.58093417e+02]\n",
      " [3.29553357e-01 1.70061564e+02 1.22240302e+01 ... 2.48492144e-03\n",
      "  6.68821817e-02 7.31796637e+02]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20000, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(param_save)\n",
    "param_save.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#normalize the rows of Obs_sim to have NOS in row 1 and NSS in the remaining rows. Substract rows i=2:NUMBER_SIMS from row 1 of Obs_sim (whic contain OS).Compute the eucleadean distance (d) between NSS and NOS then use it along side tolerance (δ), to determine all parameters and NSS corresponding to d ≤ δ.Choose δ such that δ × 100% of the NUMBER_SIMS simulated parameters and NSS are selected. retain the parameters that made this threshold (library), the weights ot be used in local linear regression and the NSS that meets the threshold (stats)\n",
    "library, dists, stats,stats_SS,  NSS_cutoff, library_index   = sum_stats(Obs_Sim, param_save) # CURRENTLY FAILING HERE BECAUSE OBS_SIM INCLUDES SOME NA VALUES, LEADING NORMALIZE() TO FAIL\n",
    "# performing rejectio ABC. Note that if UMBER_SIMS is big enough, but rejection and regression ABC leads to teh same results.\n",
    "if abc_pref=='rejection':\n",
    "    result, HPDR=do_rejection(library)\n",
    "    print('see the results below')\n",
    "    print('Estimates from rejection is:', result)\n",
    "    print('Estimated HPDR from rejection is :', HPDR)\n",
    "# Next we have regression ABC, perform it if only you are not performing rejection ABC above. Gives better results for NUMBER_SIMS small. I have commented it.\n",
    "#  if abc_pref=='regression':\n",
    "#      library_reg=do_logit_transformation(library, param_bound)LJ=34, Ly=68, Linf=200 # AFH: still commented out because the code is a little messed up\n",
    "#      result_reg, HPDR_reg=do_kernel_ridge(stats, library_reg, param_bound)\n",
    "PARAMS1={}\n",
    "print(result[2]) # what are each of these? can we add a name column? \n",
    "PARAMS1 = {\"L_0\":result[0] , \"L_inf\": result[1],\"L_J\": 39.75,\"L_Y\": 67.5, \"Topt\": result[2], \"width\": result[3], \"kopt\": result[4],\"xi\":result[5], \"m_J\": result[6], \"m_Y\":result[7] , \"m_A\": result[8], \"K\": result[9]}\n",
    "\n",
    "N_J1, N_Y1, N_A1 = simulation_population(PARAMS1) #AF: it appears that N_J1 contains the simulations for juveniles--all of them, not just patch 1? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save parameter data \n",
    "import csv\n",
    "with open('parameter_estimates.csv', 'w') as csv_file:  \n",
    "    writer = csv.writer(csv_file)\n",
    "    for key, value in PARAMS1.items():\n",
    "       writer.writerow([key, value])\n",
    "# sheer wizardry "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to write out results of model to plot in R \n",
    "#print(type(N_J1))\n",
    "#testdf = pd.DataFrame(N_J1)\n",
    "\n",
    "#print(nn)\n",
    "#print(testdf) # I think this has patch as columns and years as rows. would be nice to preserve the real year and patch IDs \n",
    "\n",
    "# Jude's code to write out to df \n",
    "\n",
    "print(PARAMS1) # write this out too! \n",
    "\n",
    "yr=[]\n",
    "lat=[]\n",
    "stage=[]\n",
    "abun=[]\n",
    "for p in range(0, latN): \n",
    "    for q in range(0, yearsTestN):\n",
    "        abun.append(N_J1[:,p][q]) # N_J1 I think has a row for each year, both training and testing. Is this getting the correct year?? \n",
    "        lat.append(latRange.min()+p) #lat.append(36+p)\n",
    "        yr.append(year_split+q+1) #yr.append(2013+q) # adding 1 because I'm afraid to start range() at 1 in case it messes up the indexing\n",
    "        stage.append('smalljuv')\n",
    "        abun.append(N_Y1[:,p][q])\n",
    "        lat.append(latRange.min()+p) #lat.append(36+p)\n",
    "        yr.append(year_split+q+1) #yr.append(2013+q)\n",
    "        stage.append('largejuv')\n",
    "        abun.append(N_A1[:,p][q])\n",
    "        lat.append(latRange.min()+p) #lat.append(36+p)\n",
    "        yr.append(year_split+q+1) #yr.append(2013+q)\n",
    "        stage.append('adult')\n",
    "df=pd.DataFrame({'Year': yr, 'Latitude':lat, 'Stage': stage, 'Abundance': abun})\n",
    "print(df)\n",
    "\n",
    "df.to_csv('spiny_dogfish_out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Importing a file call plot to plot the results.\n",
    "# # print('i just imported a plot')\n",
    "# for q in range(1,no_patches+1):\n",
    "#     i=q-1\n",
    "#     p=q\n",
    "#     plot.do_realdata(N_J1[:,i], N_J[:,i],  'J_abun_rej'+ str(p))\n",
    "#     #plot.do_scatter(N_J1[:,i], N_J[:,i],  'J_abun_scatter'+ str(p))\n",
    "#     plot.do_realdata(N_Y1[:,i], N_Y[:,i],  'Y_abun_rej'+ str(p))\n",
    "#     #plot.do_scatter(N_Y1[:,i], N_Y[:,i],  'Y_abun_scatter'+ str(p))\n",
    "#     plot.do_realdata(N_A1[:,i], N_A[:,i],  'A_abun_rej'+ str(p))\n",
    "# #plot.do_scatter(N_A1[:,i], N_A[:,i],  'A_abun_scatter'+ str(p))\n",
    "# ################################################################\n",
    "# # plot the figures below if you willl like to plot the heatmap\n",
    "#     NJ1=N_J1.transpose()\n",
    "#     NJ=N_J.transpose()\n",
    "#     NY1=N_Y1.transpose()\n",
    "#     NY=N_Y.transpose()\n",
    "#     NA1=N_A1.transpose()\n",
    "#     NA=N_A.transpose()\n",
    "#     print(NJ1.shape)\n",
    "#     ax=sns.heatmap(NJ1, cmap=\"Greys\", xticklabels=True, yticklabels=True,  cbar_kws={'label': 'Abundance'})\n",
    "#     plt.xlabel(\"Year\")\n",
    "#     plt.ylabel(\"Latitude\")\n",
    "#     ax.set_xticklabels(pd.Series(range(1980, 2012)))\n",
    "#     ax.set_yticklabels(pd.Series(range(34, 46)))\n",
    "#     ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "#     ax.figure.savefig(\"sim_J.png\", bbox_inches='tight')\n",
    "#     plt.close()\n",
    "# #############################################################\n",
    "#     ax = sns.heatmap(NJ, cmap=\"Greys\",  xticklabels=True, yticklabels=True, cbar_kws={'label': 'Abundance'})\n",
    "#     plt.xlabel(\"Year\")\n",
    "#     plt.ylabel(\"Latitude\")\n",
    "#     ax.set_xticklabels(pd.Series(range(1980, 2012)))\n",
    "#     ax.set_yticklabels(pd.Series(range(34, 46)))\n",
    "#     ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "#     ax.figure.savefig(\"Obs_J.png\", bbox_inches='tight')\n",
    "#     plt.close()\n",
    "# ##########################################################\n",
    "#     ax = sns.heatmap(NY1, cmap=\"Greys\", xticklabels=True, yticklabels=True,  cbar_kws={'label': 'Abundance'})\n",
    "#     plt.xlabel(\"Year\")\n",
    "#     plt.ylabel(\"Latitude\")\n",
    "#     ax.set_xticklabels(pd.Series(range(1980, 2013)))\n",
    "#     ax.set_yticklabels(pd.Series(range(34, 46)))\n",
    "#     ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "#     ax.figure.savefig(\"Sim_Y.png\", bbox_inches='tight')\n",
    "#     plt.close()\n",
    "# ##########################################################\n",
    "#     ax = sns.heatmap(NY, cmap=\"Greys\", xticklabels=True, yticklabels=True,  cbar_kws={'label': 'Abundance'})\n",
    "#     plt.xlabel(\"Year\")\n",
    "#     plt.ylabel(\"Latitude\")\n",
    "#     ax.set_xticklabels(pd.Series(range(1980, 2013)))\n",
    "#     ax.set_yticklabels(pd.Series(range(34, 46)))\n",
    "#     ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "#     ax.figure.savefig(\"Obs_Y.png\", bbox_inches='tight')\n",
    "#     plt.close()\n",
    "# ############################################################\n",
    "#     ax = sns.heatmap(NA1, cmap=\"Greys\", xticklabels=True, yticklabels=True, cbar_kws={'label': 'Abundance'})\n",
    "#     plt.xlabel(\"Year\")\n",
    "#     plt.ylabel(\"Latitude\")\n",
    "#     ax.set_xticklabels(pd.Series(range(1980, 2013)))\n",
    "#     ax.set_yticklabels(pd.Series(range(34, 46)))\n",
    "#     ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "#     ax.figure.savefig(\"Sim_A.png\", bbox_inches='tight')\n",
    "#     plt.close()\n",
    "# ############################################################\n",
    "#     ax = sns.heatmap(NA, cmap=\"Greys\",  xticklabels=True, yticklabels=True, cbar_kws={'label': 'Abundance'})\n",
    "#     plt.xlabel(\"Year\")\n",
    "#     plt.ylabel(\"Latitude\")\n",
    "#     ax.set_xticklabels(pd.Series(range(1980, 2013)))\n",
    "#     ax.set_yticklabels(pd.Series(range(34, 46)))\n",
    "#     ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "#     ax.figure.savefig(\"Obs_A.png\", bbox_inches='tight')\n",
    "#     plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
