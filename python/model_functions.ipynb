{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Range Model Functions\n",
    "\n",
    "## Description\n",
    "\n",
    "This script contains functions to implement a stage-structured, process-based, spatially explicit single-species population model. Users should be able to import this file and then run the model independently using real or simulated survey data. \n",
    "\n",
    "## Authors\n",
    "\n",
    "This model was developed by Jude Kong and revised by Alexa Fredston-Hermann, in collaboration with Malin Pinsky. \n",
    "\n",
    "## References \n",
    "\n",
    "Please refer to this manuscript for more details: J. D. Kong, M. Pinsky, E. Moberg, and B. Selden. Using stage structure to infer process-based models for species on the move. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import numpy as np\n",
    "import math as math\n",
    "import random as random\n",
    "import sys\n",
    "import copy as copy\n",
    "from scipy.stats import norm\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from numpy.linalg import inv\n",
    "import pymc3\n",
    "import scipy.stats as sst\n",
    "from sklearn import preprocessing\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.kernel_ridge import KernelRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_dependence(temperature, Topt, width, kopt):\n",
    "    \"\"\" compute growth rate as a function of temperature, were kopt is the optimal growth rate, Topt, optimal temperature, width, the standard deviation from the optimal temperature.\n",
    "        \"\"\"\n",
    "    #theta = -width*(temperature-Topt)*(temperature-Topt) + kopt\n",
    "    theta = kopt*np.exp(-0.5*np.square((temperature-Topt)/width))\n",
    "    #theta = kopt*(1-(np.square((temperature-Topt)/width)))\n",
    "    #theta=((temperature-Tmin)*(temperature-Tmax))/(((temperature-Tmin)*(temperature-Tmax))-(temperature-Topt))\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation_population(params):\n",
    "    \"\"\" Takes in the initial population sizes and simulates the population size moving forward \"\"\"\n",
    "    # Set starting numbers for population and allocate space for population sizes\n",
    "    N_JS = np.ndarray(shape=(rows, cols), dtype=float, order='F')\n",
    "    alpha=N_JS.copy()\n",
    "    N_YS=N_JS.copy()\n",
    "    N_AS=N_JS.copy()\n",
    "    for x in range(0, cols):\n",
    "        alpha[:,x]=temp_dependence(tempA[:,x],  params[\"Topt\"], params[\"width\"], params[\"kopt\"])\n",
    "    N_JS[0,:]=N_J[0,:]\n",
    "    N_YS[0,:]=N_Y[0,:]\n",
    "    N_AS[0,:]=N_A[0,:]\n",
    "    L_bJ=params[\"L_inf\"]-(params[\"L_inf\"]-params[\"L_J\"])*np.exp(alpha)\n",
    "    L_bY=params[\"L_inf\"]-(params[\"L_inf\"]-params[\"L_Y\"])*np.exp(alpha)\n",
    "    g_J=(params[\"L_J\"]-L_bJ)/(params[\"L_J\"]-params[\"L_0\"])\n",
    "    g_Y=(params[\"L_Y\"]-L_bY)/(params[\"L_Y\"]-params[\"L_J\"])\n",
    "    for t in range(0,rows-1):\n",
    "        #boundary values\n",
    "\n",
    "        N_JS[t+1,0]=max(0,(1-params[\"m_J\"])*N_JS[t,0]+(1-params[\"xi\"])*N_AS[t,0]*np.exp(alpha[t,0]*(1-N_AS[t,0]/params[\"K\"]))-g_J[t,0]*N_JS[t,0]+ params[\"xi\"]*N_AS[t,1]*np.exp(alpha[t,1]*(1-N_AS[t,1]/params[\"K\"])))\n",
    "        N_JS[t+1,cols-1]=max(0,(1-params[\"m_J\"])*N_JS[t,cols-1]+(1-params[\"xi\"])*N_AS[t,cols-1]*np.exp(alpha[t,cols-1]*(1-N_AS[t,cols-1]/params[\"K\"]))+ params[\"xi\"]*N_AS[t,cols-2]*np.exp(alpha[t,cols-2]*(1-N_AS[t,cols-2]/params[\"K\"]))-g_J[t,cols-1]*N_JS[t,cols-1])\n",
    "        N_YS[t+1, 0] = max(0,(1-params[\"m_Y\"])*N_YS[t,0]+g_J[t,0]*N_JS[t,0]-g_Y[t,0]*N_YS[t,0])\n",
    "        N_YS[t+1, cols-1] = max(0,(1-params[\"m_Y\"])*N_YS[t,cols-1]+g_J[t, cols-1]*N_JS[t,cols-1]-g_Y[t,cols-1]*N_YS[t,cols-1])\n",
    "        \n",
    "        N_AS[t+1, 0]=max(0,(1-params[\"m_A\"])*N_AS[t,0]+g_Y[t,0]*N_YS[t,0]-params[\"xi\"]*N_AS[t,0]+ params[\"xi\"]*N_AS[t,1])\n",
    "        N_AS[t+1, cols-1]=max(0,(1-params[\"m_A\"])*N_AS[t,cols-1]+g_Y[t,cols-1]*N_YS[t,cols-1]-params[\"xi\"]*N_AS[t,cols-1]+ params[\"xi\"]*N_AS[t,cols-2])\n",
    "        for x in range(1, cols-1):\n",
    "            N_JS[t+1, x]=max(0,(1-params[\"m_J\"])*N_JS[t,x]-g_J[t,x]*N_JS[t,x]+(1-2*params[\"xi\"])*N_AS[t,x]*np.exp(alpha[t,x]*(1-N_AS[t,x]/params[\"K\"]))+ params[\"xi\"]*N_AS[t,x+1]*np.exp(alpha[t,x+1]*(1-N_AS[t,x+1]/params[\"K\"]))+ params[\"xi\"]*N_AS[t,x-1]*np.exp(alpha[t,x-1]*(1-N_AS[t,x-1]/params[\"K\"])))\n",
    "            N_YS[t+1, x] = max(0,(1-params[\"m_Y\"])*N_YS[t,x]+g_J[t,x]*N_JS[t,x]-g_Y[t,x]*N_YS[t,x])\n",
    "            N_AS[t+1, x]=max(0,(1-params[\"m_A\"])*N_AS[t,x]+g_Y[t,x]*N_YS[t,x]-2*params[\"xi\"]*N_AS[t,x]+ params[\"xi\"]*N_AS[t,x+1]+params[\"xi\"]*N_AS[t,x-1])\n",
    "    return N_JS, N_YS, N_AS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caluclate summary statistics for the oberve data\n",
    "def calculate_summary_stats(N_J, N_Y, N_A):\n",
    "    \"\"\"Takes in a matrix of time x place population sizes for each stage and calculates summary statistics\"\"\"\n",
    "    time=range(T_FINAL)\n",
    "    L_Q1=np.percentile(time, 5, axis=None, out=None, overwrite_input=False, interpolation='nearest')\n",
    "    L_Q=np.percentile(time, 25, axis=None, out=None, overwrite_input=False, interpolation='nearest')\n",
    "    M_Q=np.percentile(time, 50, axis=None, out=None, overwrite_input=False, interpolation='nearest')\n",
    "    M_Q1=np.percentile(time,60, axis=None, out=None, overwrite_input=False,interpolation='nearest')\n",
    "    U_Q=np.percentile(time, 75, axis=None, out=None, overwrite_input=False, interpolation='nearest')\n",
    "    U_Q1=np.percentile(time, 80, axis=None, out=None, overwrite_input=False, interpolation='nearest')\n",
    "    U_Q2=np.percentile(time, 85, axis=None, out=None, overwrite_input=False, interpolation='nearest')\n",
    "    U_Q3=np.percentile(time, 90, axis=None, out=None, overwrite_input=False, interpolation='nearest')\n",
    "    U_Q4=np.percentile(time,95, axis=None, out=None, overwrite_input=False,interpolation='nearest')\n",
    "    U_Q5=np.percentile(time, 97, axis=None, out=None, overwrite_input=False, interpolation='nearest')\n",
    "    U_Q6=np.percentile(time, 99, axis=None, out=None, overwrite_input=False, interpolation='nearest')\n",
    "    SS_adult1=np.hstack((N_A[L_Q1], N_A[L_Q], N_A[M_Q],N_A[M_Q1], N_A[U_Q],N_A[U_Q1],N_A[U_Q2],N_A[U_Q3],N_A[U_Q4], N_A[U_Q5], N_A[U_Q6]))#np.std(N_A, axis=0), np.mean(N_A, axis=0), sst.skew(N_A, axis=0, bias=True),\n",
    "    SS_young1=np.hstack((N_Y[L_Q1], N_Y[L_Q], N_Y[M_Q],N_Y[M_Q1], N_Y[U_Q],N_Y[U_Q1], N_Y[U_Q2], N_Y[U_Q3], N_Y[U_Q4], N_Y[U_Q5], N_Y[U_Q6]))#np.std(N_Y, axis=0), np.mean(N_Y, axis=0), bias=True),\n",
    "    SS_juv1=np.hstack((N_J[L_Q1], N_J[L_Q], N_J[M_Q],N_J[M_Q1], N_J[U_Q],N_J[U_Q1], N_J[U_Q2], N_J[U_Q3], N_J[U_Q4], N_J[U_Q5], N_J[U_Q6]))#np.std(N_J, axis=0), np.mean(N_J, axis=0), axis=0, bias=True),\n",
    "    #SS_adult1=np.hstack((N_A[L_Q1], N_A[L_Q], N_A[M_Q],N_A[M_Q1], N_A[U_Q]), N_A[U_Q4]))\n",
    "    #SS_young1=np.hstack((N_Y[L_Q1], N_Y[L_Q], N_Y[M_Q],N_Y[M_Q1], N_Y[U_Q]), N_Y[U_Q4]))\n",
    "    #SS_juv1=np.hstack(( N_Y[L_Q1], N_Y[L_Q], N_Y[M_Q],N_Y[M_Q1], N_Y[U_Q], N_Y[U_Q4]))\n",
    "    #SS_adult1=np.hstack((N_A[L_Q], N_A[M_Q], N_A[U_Q]))\n",
    "    #SS_young1=np.hstack((N_Y[L_Q], N_Y[M_Q], N_Y[U_Q]))\n",
    "    #SS_juv1=np.hstack((N_Y[L_Q], N_Y[M_Q], N_Y[U_Q]))\n",
    "    # SS_adult1=np.hstack((N_A))\n",
    "    #SS_young1=np.hstack((N_Y))\n",
    "    #SS_juv1=np.hstack(( N_J))\n",
    "    return SS_adult1, SS_young1, SS_juv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_percent(vector, percent):\n",
    "    \"\"\" Takes a vector and returns the indexes of the elements within the smallest (percent) percent of the vector\"\"\"\n",
    "    sorted_vector = sorted(vector)\n",
    "    cutoff = math.floor(len(vector)*percent/100) # finds the value which (percent) percent are below\n",
    "    indexes = []\n",
    "    \n",
    "    cutoff = int(cutoff)\n",
    "\n",
    "    for i in range(0,len(vector)):\n",
    "        if vector[i] <= sorted_vector[cutoff]: # looks for values below the cutoff\n",
    "            indexes.append(i)\n",
    "    return indexes, sorted_vector[cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(x):\n",
    "    \"\"\"Takes a list and returns a 0 centered, std = 1 scaled version of the list\"\"\"\n",
    "    st_dev = np.std(x,axis=0)\n",
    "    mu = np.mean(x,axis=0)\n",
    "    rescaled_values = []\n",
    "    for element in range(0,len(x)):\n",
    "        rescaled_values[element] = (x[element] - mu) / st_dev\n",
    "\n",
    "    return rescaled_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function transform the paramters using logit function. the aim is to ensure that we do not end up with a parameter out of the prior\n",
    "def do_logit_transformation(library, param_bound):\n",
    "    for i in range(len(library[0,:])):\n",
    "        library[:,i]=(library[:,i]-param_bound[i,0])/(param_bound[i,1]-param_bound[i,0])\n",
    "        library[:,i]=np.log(library[:,i]/(1-library[:,i]))\n",
    "    return library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function back transform parameter values\n",
    "def do_ivlogit_transformation(para_reg, param_bound):\n",
    "    for i in range(len(library[0,:])):\n",
    "        para_reg[:,i]=np.exp(para_reg[:,i])/(1+np.exp(para_reg[:,i]))\n",
    "        para_reg[:,i]=para_reg[:,i]*(param_bound[i,1]-param_bound[i,0])+param_bound[i,0]\n",
    "    return para_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_kernel_ridge(stats, library, param_bound):\n",
    "    \"This function transforms the accepted parameter values using local linear regression\"\n",
    "    X = sm.add_constant(stats)\n",
    "    Y=library\n",
    "    clf     = KernelRidge(alpha=1.0, kernel='rbf', coef0=1)\n",
    "    resul   = clf.fit(X, Y)\n",
    "    resul_coef=np.dot(X.transpose(), resul.dual_coef_)\n",
    "    coefficients =resul_coef[1:]\n",
    "    para_reg   =Y- stats.dot(coefficients)\n",
    "    para_reg=do_ivlogit_transformation(para_reg, param_bound)\n",
    "    parameter_estimate = np.average(para_reg, axis=0)\n",
    "    HPDR=pymc3.stats.hpd(para_reg)\n",
    "    return parameter_estimate, HPDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_rejection(library):\n",
    "    \"This function performs rejection ABC\"\n",
    "    parameter_estimate = np.average(library, axis=0)\n",
    "    HPDR=pymc3.stats.hpd(library)\n",
    "    return parameter_estimate, HPDR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sim():\n",
    "    PARAMS_ABC = {}# copies parameters so new values can be generated;\n",
    "    #an array for the parameters that wil be generated using ABC\n",
    "    param_save = []\n",
    "    #Calculate summary stastistics for the observed data\n",
    "    SS_adult, SS_young, SS_juv= calculate_summary_stats(N_J, N_Y, N_A)\n",
    "    \n",
    "    SO=np.hstack((SS_adult, SS_young, SS_juv))\n",
    "    Obs_Sim=np.zeros((NUMBER_SIMS+1,len(SO)))\n",
    "    Obs_Sim[0,:]=SO\n",
    "    #simulaate candidate parameters\n",
    "    for i in range(0,NUMBER_SIMS):\n",
    "        L_0_theta    = np.random.uniform(0,4)#np.random.normal(0.4,0.3) #np.random.beta(2,2)\n",
    "        L_inf_theta    =np.random.uniform(100, 180)\n",
    "        L_J_theta=39.75\n",
    "        L_Y_theta=67.5\n",
    "        #np.random.uniform(0,1)#np.random.beta(2,2)\n",
    "        Topt_theta =np.random.uniform(6,15)#np.random.normal(6.5,2) #np.random.uniform(1,12) #np.random.lognormal(1,1)\n",
    "        width_theta  =np.random.uniform(1,2)#np.random.normal(2,1)\n",
    "        ##np.random.lognormal(1,1)\n",
    "        kopt_theta    =np.random.uniform(0.1,1)#np.random.normal(0.5,0.4)# np.random.u(0,1)\n",
    "        xi_theta     =np.random.uniform(0,0.5/2)#np.random.normal(0.1,0.09) #np.random.normal(0,1)#np.random.normal(0,0.5)\n",
    "        #r_theta     =np.random.uniform(0,1)\n",
    "        m_J_theta    =np.random.uniform(0,0.1)#np.random.normal(0.04,0.04) # #np.random.beta(2,2)\n",
    "        m_Y_theta    =np.random.uniform(0,0.1)#np.random.normal(0.05,0.04) #np.random.uniform(0,1) #np.random.beta(2,2)\n",
    "        m_A_theta    =np.random.uniform(0,0.1)#np.random.normal(0.05,0.05)# np.random.uniform(0,1)#np.random.beta(2,2)\n",
    "        K_theta= np.random.uniform(1,1000)\n",
    "        PARAMS_ABC[\"L_0\"]    = L_0_theta # s\n",
    "        PARAMS_ABC[\"L_inf\"]    =L_inf_theta\n",
    "        PARAMS_ABC[\"L_J\"]    =  L_J_theta\n",
    "        PARAMS_ABC[\"L_Y\"]    = L_Y_theta\n",
    "        PARAMS_ABC[\"Topt\"] = Topt_theta\n",
    "        PARAMS_ABC[\"width\"]  = width_theta\n",
    "        PARAMS_ABC[\"kopt\"]    = kopt_theta\n",
    "        PARAMS_ABC[\"xi\"]     = xi_theta\n",
    "        #PARAMS_ABC[\"r\"]     = r_theta\n",
    "        PARAMS_ABC[\"m_J\"]    = m_J_theta\n",
    "        PARAMS_ABC[\"m_Y\"]    = m_Y_theta\n",
    "        PARAMS_ABC[\"m_A\"]    = m_A_theta\n",
    "        PARAMS_ABC[\"K\"]    = K_theta\n",
    "        # Simulate population for new parameters\n",
    "        N_J_sim, N_Y_sim, N_A_sim = simulation_population(PARAMS_ABC)\n",
    "        \n",
    "        # Calculate the summary statistics for the simulation\n",
    "        Sim_SS_adult, Sim_SS_young, Sim_SS_juv= calculate_summary_stats(N_J_sim, N_Y_sim, N_A_sim)\n",
    "        SS=np.hstack((Sim_SS_adult, Sim_SS_young, Sim_SS_juv))\n",
    "        Obs_Sim[i+1,:]=SS\n",
    "        # Saving candidate parameter values\n",
    "        param_save.append([L_0_theta,L_inf_theta, Topt_theta, width_theta, kopt_theta, xi_theta, m_J_theta,m_Y_theta, m_A_theta, K_theta])\n",
    "    \n",
    "    return np.asarray(param_save), Obs_Sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return all  all parameters (library) and simulated  NSS (stats) corresponding to d ≤ δ(eps).\n",
    "def compute_scores(dists, param_save, difference,Sim_SS):\n",
    "    eps=0.1\n",
    "    library_index, NSS_cutoff = small_percent(dists, eps)\n",
    "    n                = len(library_index)\n",
    "    library = np.empty((n, param_save.shape[1]))\n",
    "    stats            = np.empty((n, difference.shape[1]))\n",
    "    stats_SS            = np.empty((n, difference.shape[1]))\n",
    "    for i in range(0,len(library_index)):\n",
    "        j = library_index[i]\n",
    "        library[i] = param_save[j]\n",
    "        stats[i]   = difference[j]\n",
    "        stats_SS[i]   = Sim_SS[j]\n",
    "    return library, stats, NSS_cutoff, library_index, stats_SS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computes weights for local regression\n",
    "\n",
    "def compute_weight(kernel,t, eps, index):\n",
    "     weights=np.empty(len(index))\n",
    "     if (kernel == \"epanechnikov\"):\n",
    "         for i in range(0,len(library_index)):\n",
    "             j = library_index[i]\n",
    "     #weights[i]= (1. - (t[j] / eps)**2)\n",
    "             weights[i]=(1. - (t[j] / eps)**2)\n",
    "     elif(kernel == \"rectangular\"):\n",
    "          for i in range(0,len(library_index)):\n",
    "              j = library_index[i]\n",
    "              weights[i]=t[j] / eps\n",
    "     elif (kernel == \"gaussian\"):\n",
    "          for i in range(0,len(library_index)):\n",
    "              j = library_index[i]\n",
    "              weights[i]= 1/np.sqrt(2*np.pi)*np.exp(-0.5*(t[j]/(eps/2))**2)\n",
    "            \n",
    "     elif (kernel == \"triangular\"):\n",
    "          for i in range(0,len(library_index)):\n",
    "              j = library_index[i]\n",
    "              weights[i]= 1 - np.abs(t[j]/eps)\n",
    "     elif (kernel == \"biweight\"):\n",
    "          for i in range(0,len(library_index)):\n",
    "              j = library_index[i]\n",
    "              weights[i]=(1 - (t[j]/eps)**2)**2\n",
    "     else:\n",
    "          for i in range(0,len(library_index)):\n",
    "              j = library_index[i]\n",
    "              weights[i]= np.cos(np.pi/2*t[j]/eps)\n",
    "     return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_stats(Obs_Sim, param_save):\n",
    "    dists = np.zeros((NUMBER_SIMS,1))\n",
    "    #Obs_Sim_scale=np.nan_to_num(sst.zscore(Obs_Sim, axis=0,ddof=1),copy=True)\n",
    "    Obs_Sim_scale=np.nan_to_num(preprocessing.normalize(Obs_Sim, axis=0),copy=True)\n",
    "    #Substract each row of teh array from row 1\n",
    "    Sim_SS=Obs_Sim_scale[1:NUMBER_SIMS+1,: ]\n",
    "    Obs_SS=Obs_Sim_scale[0,:]\n",
    "    difference=Obs_Sim_scale[1:NUMBER_SIMS+1,: ]-Obs_Sim_scale[0,:]\n",
    "    #c=np.std(Obs_Sim_scale[1:NUMBER_SIMS+1,: ], axis=1)\n",
    "    # compute the norm 2 of each row\n",
    "    dists = np.linalg.norm(difference, axis=1)\n",
    "    library, stats, NSS_cutoff, library_index, stats_SS = compute_scores(dists, param_save, difference,Sim_SS)\n",
    "    # print(library)\n",
    "    return library, dists, stats,stats_SS,   NSS_cutoff, library_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_regression(library, stats, PARAMS):\n",
    "\n",
    "    # REJECTION\n",
    "    print('\\nDo a rejection ABC:')\n",
    "    do_rejection(library, PARAMS)\n",
    "    do_local_linear(stats, library, weights,KK)\n",
    "    #print('\\nStats:', stats.shape)\n",
    "    #print('\\nStats:', stats)\n",
    "    #print('\\nLibar:', library.shape)\n",
    "    #print('\\nLibar:', library)\n",
    "\n",
    "    do_kernel_ridge(stats, library)\n",
    "    do_ridge(stats, library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_goodness_fit(result,HPDR, actual, n, i):\n",
    "    for j in range(0,n):\n",
    "        if HPDR[j][0]<=actual[j]<=HPDR[j][1]:\n",
    "           coverage[i,j]=1\n",
    "        else:\n",
    "           coverage[i,j]=0\n",
    "    resultsbias[i,:] = (result - actual)/actual\n",
    "    return coverage,resultsbias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
